{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "white_wine=pd.read_csv('../../practice/winequality-white.csv', sep=';')\n",
    "red_wine=pd.read_csv('../../practice/winequality-red.csv', sep=';')\n",
    "white_wine.columns=white_wine.columns.str.replace(' ','_')\n",
    "red_wine.columns=red_wine.columns.str.replace(' ','_')\n",
    "white_wine['wine_type']=0\n",
    "red_wine['wine_type']=1\n",
    "total_wine=pd.concat([white_wine, red_wine])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>wine_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.00100</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.99400</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.99510</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  \\\n",
       "0               7.0             0.270         0.36            20.7      0.045   \n",
       "1               6.3             0.300         0.34             1.6      0.049   \n",
       "2               8.1             0.280         0.40             6.9      0.050   \n",
       "3               7.2             0.230         0.32             8.5      0.058   \n",
       "4               7.2             0.230         0.32             8.5      0.058   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free_sulfur_dioxide  total_sulfur_dioxide  density    pH  sulphates  \\\n",
       "0                    45.0                 170.0  1.00100  3.00       0.45   \n",
       "1                    14.0                 132.0  0.99400  3.30       0.49   \n",
       "2                    30.0                  97.0  0.99510  3.26       0.44   \n",
       "3                    47.0                 186.0  0.99560  3.19       0.40   \n",
       "4                    47.0                 186.0  0.99560  3.19       0.40   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  wine_type  \n",
       "0         8.8        6          0  \n",
       "1         9.5        6          0  \n",
       "2        10.1        6          0  \n",
       "3         9.9        6          0  \n",
       "4         9.9        6          0  \n",
       "...       ...      ...        ...  \n",
       "1594     10.5        5          1  \n",
       "1595     11.2        6          1  \n",
       "1596     11.0        6          1  \n",
       "1597     10.2        5          1  \n",
       "1598     11.0        6          1  \n",
       "\n",
       "[6497 rows x 13 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sI use z score method to remove outliers\n",
    "def outliers_remov(df):\n",
    "    for colu in total_wine:\n",
    "        mean_df=df[colu].mean()\n",
    "        std_df=df[colu].std()\n",
    "        df_out= df[(df[colu] <= 3*std_df+mean_df) & (df[colu] >= (mean_df-3*std_df))]\n",
    "        df=df_out     \n",
    "    return df_out\n",
    "\n",
    "\n",
    "\n",
    "total_wine=outliers_remov(total_wine)\n",
    "total_wine= total_wine.set_index(np.arange(len(total_wine)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score :\n",
      " 0.9936936936936936 \n",
      "cohen: \n",
      " 0.9811825484482561 \n",
      "confusion \n",
      " [array([[1410,    6],\n",
      "       [   5,  355]]), array([[1399,    6],\n",
      "       [   4,  367]]), array([[1386,    2],\n",
      "       [  11,  377]]), array([[1372,    2],\n",
      "       [   9,  393]]), array([[1381,    0],\n",
      "       [  11,  384]]), array([[1403,    3],\n",
      "       [   9,  361]]), array([[1379,    2],\n",
      "       [   6,  389]]), array([[1391,    4],\n",
      "       [  10,  371]]), array([[1402,    4],\n",
      "       [  10,  360]]), array([[1388,    1],\n",
      "       [   7,  380]])] classification: \n",
      " ['              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1416\\n           1       0.98      0.99      0.98       360\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1405\\n           1       0.98      0.99      0.99       371\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      1.00      1.00      1388\\n           1       0.99      0.97      0.98       388\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      1.00      1.00      1374\\n           1       0.99      0.98      0.99       402\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      1.00      1.00      1381\\n           1       1.00      0.97      0.99       395\\n\\n    accuracy                           0.99      1776\\n   macro avg       1.00      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      1.00      1.00      1406\\n           1       0.99      0.98      0.98       370\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1381\\n           1       0.99      0.98      0.99       395\\n\\n    accuracy                           1.00      1776\\n   macro avg       1.00      0.99      0.99      1776\\nweighted avg       1.00      1.00      1.00      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      1.00      0.99      1395\\n           1       0.99      0.97      0.98       381\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      1.00      1.00      1406\\n           1       0.99      0.97      0.98       370\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      1.00      1.00      1389\\n           1       1.00      0.98      0.99       387\\n\\n    accuracy                           1.00      1776\\n   macro avg       1.00      0.99      0.99      1776\\nweighted avg       1.00      1.00      1.00      1776\\n']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cohen=[]\n",
    "score=[]\n",
    "confusion=[]\n",
    "class_out=[]\n",
    "for i in range(10): \n",
    "    model=LogisticRegression()\n",
    "    x=total_wine.drop(columns='wine_type')\n",
    "    y=total_wine['wine_type']\n",
    "    x_train,x_test,y_train, y_test=train_test_split(x,y, test_size=0.3)\n",
    "    scale=MinMaxScaler()\n",
    "    x_train_n= scale.fit_transform(x_train)\n",
    "    x_train_n= pd.DataFrame(x_train_n, columns=x_train.columns)\n",
    "    x_test_n= scale.fit_transform(x_test)\n",
    "    x_test_n= pd.DataFrame(x_test_n, columns=x_test.columns)\n",
    "    model.fit(x_train_n, y_train)\n",
    "    pred=model.predict(x_test_n)\n",
    "    scor=accuracy_score(pred, y_test)\n",
    "    score.append(scor)\n",
    "\n",
    "    con=confusion_matrix(y_test, pred)\n",
    "    confusion.append(con)\n",
    "    class_p= classification_report(y_test,pred)\n",
    "    class_out.append(class_p)\n",
    "    cohe=cohen_kappa_score(y_test, pred)\n",
    "    cohen.append(cohe)\n",
    "    i +=1\n",
    "print('score :\\n', np.mean(score),'\\ncohen: \\n',np.mean(cohen),'\\nconfusion \\n',confusion, 'classification: \\n',class_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score :\n",
      " 0.9763513513513514 \n",
      "cohen: \n",
      " 0.9295262423581973 \n",
      "confusion \n",
      " [array([[1377,   39],\n",
      "       [  18,  342]]), array([[1375,   21],\n",
      "       [  16,  364]]), array([[1362,   13],\n",
      "       [  23,  378]]), array([[1375,   19],\n",
      "       [  20,  362]]), array([[1383,   15],\n",
      "       [  23,  355]]), array([[1378,   23],\n",
      "       [  20,  355]]), array([[1363,   34],\n",
      "       [  19,  360]]), array([[1362,   12],\n",
      "       [  27,  375]]), array([[1409,   12],\n",
      "       [  21,  334]]), array([[1376,   22],\n",
      "       [  23,  355]])] classification: \n",
      " ['              precision    recall  f1-score   support\\n\\n           0       0.99      0.97      0.98      1416\\n           1       0.90      0.95      0.92       360\\n\\n    accuracy                           0.97      1776\\n   macro avg       0.94      0.96      0.95      1776\\nweighted avg       0.97      0.97      0.97      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      0.98      0.99      1396\\n           1       0.95      0.96      0.95       380\\n\\n    accuracy                           0.98      1776\\n   macro avg       0.97      0.97      0.97      1776\\nweighted avg       0.98      0.98      0.98      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.98      0.99      0.99      1375\\n           1       0.97      0.94      0.95       401\\n\\n    accuracy                           0.98      1776\\n   macro avg       0.98      0.97      0.97      1776\\nweighted avg       0.98      0.98      0.98      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      0.99      0.99      1394\\n           1       0.95      0.95      0.95       382\\n\\n    accuracy                           0.98      1776\\n   macro avg       0.97      0.97      0.97      1776\\nweighted avg       0.98      0.98      0.98      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.98      0.99      0.99      1398\\n           1       0.96      0.94      0.95       378\\n\\n    accuracy                           0.98      1776\\n   macro avg       0.97      0.96      0.97      1776\\nweighted avg       0.98      0.98      0.98      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      0.98      0.98      1401\\n           1       0.94      0.95      0.94       375\\n\\n    accuracy                           0.98      1776\\n   macro avg       0.96      0.97      0.96      1776\\nweighted avg       0.98      0.98      0.98      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      0.98      0.98      1397\\n           1       0.91      0.95      0.93       379\\n\\n    accuracy                           0.97      1776\\n   macro avg       0.95      0.96      0.96      1776\\nweighted avg       0.97      0.97      0.97      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.98      0.99      0.99      1374\\n           1       0.97      0.93      0.95       402\\n\\n    accuracy                           0.98      1776\\n   macro avg       0.97      0.96      0.97      1776\\nweighted avg       0.98      0.98      0.98      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      0.99      0.99      1421\\n           1       0.97      0.94      0.95       355\\n\\n    accuracy                           0.98      1776\\n   macro avg       0.98      0.97      0.97      1776\\nweighted avg       0.98      0.98      0.98      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.98      0.98      0.98      1398\\n           1       0.94      0.94      0.94       378\\n\\n    accuracy                           0.97      1776\\n   macro avg       0.96      0.96      0.96      1776\\nweighted avg       0.97      0.97      0.97      1776\\n']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "cohen=[]\n",
    "score=[]\n",
    "confusion=[]\n",
    "class_out=[]\n",
    "for i in range(10): \n",
    "    model=DecisionTreeClassifier()\n",
    "    x=total_wine.drop(columns='wine_type')\n",
    "    y=total_wine['wine_type']\n",
    "    x_train,x_test,y_train, y_test=train_test_split(x,y, test_size=0.3)\n",
    "    scale=MinMaxScaler()\n",
    "    x_train_n= scale.fit_transform(x_train)\n",
    "    x_train_n= pd.DataFrame(x_train_n, columns=x_train.columns)\n",
    "    x_test_n= scale.fit_transform(x_test)\n",
    "    x_test_n= pd.DataFrame(x_test_n, columns=x_test.columns)\n",
    "    model.fit(x_train_n, y_train)\n",
    "    pred=model.predict(x_test_n)\n",
    "    scor=accuracy_score(pred, y_test)\n",
    "    score.append(scor)\n",
    "\n",
    "    con=confusion_matrix(y_test, pred)\n",
    "    confusion.append(con)\n",
    "    class_p= classification_report(y_test,pred)\n",
    "    class_out.append(class_p)\n",
    "    cohe=cohen_kappa_score(y_test, pred)\n",
    "    cohen.append(cohe)\n",
    "    i +=1\n",
    "print('score :\\n', np.mean(score),'\\ncohen: \\n',np.mean(cohen),'\\nconfusion \\n',confusion, 'classification: \\n',class_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score :\n",
      " 0.9938063063063064 \n",
      "cohen: \n",
      " 0.9813226811375486 \n",
      "confusion \n",
      " [array([[1413,    1],\n",
      "       [  10,  352]]), array([[1388,    4],\n",
      "       [   8,  376]]), array([[1421,    1],\n",
      "       [   9,  345]]), array([[1384,    2],\n",
      "       [  11,  379]]), array([[1432,    1],\n",
      "       [   5,  338]]), array([[1408,    2],\n",
      "       [  12,  354]]), array([[1358,    2],\n",
      "       [  11,  405]]), array([[1411,    2],\n",
      "       [   3,  360]]), array([[1402,    0],\n",
      "       [  11,  363]]), array([[1384,    2],\n",
      "       [  13,  377]])] classification: \n",
      " ['              precision    recall  f1-score   support\\n\\n           0       0.99      1.00      1.00      1414\\n           1       1.00      0.97      0.98       362\\n\\n    accuracy                           0.99      1776\\n   macro avg       1.00      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      1.00      1.00      1392\\n           1       0.99      0.98      0.98       384\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      1.00      1.00      1422\\n           1       1.00      0.97      0.99       354\\n\\n    accuracy                           0.99      1776\\n   macro avg       1.00      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      1.00      1.00      1386\\n           1       0.99      0.97      0.98       390\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1433\\n           1       1.00      0.99      0.99       343\\n\\n    accuracy                           1.00      1776\\n   macro avg       1.00      0.99      0.99      1776\\nweighted avg       1.00      1.00      1.00      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      1.00      1.00      1410\\n           1       0.99      0.97      0.98       366\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.98      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      1.00      1.00      1360\\n           1       1.00      0.97      0.98       416\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1413\\n           1       0.99      0.99      0.99       363\\n\\n    accuracy                           1.00      1776\\n   macro avg       1.00      1.00      1.00      1776\\nweighted avg       1.00      1.00      1.00      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      1.00      1.00      1402\\n           1       1.00      0.97      0.99       374\\n\\n    accuracy                           0.99      1776\\n   macro avg       1.00      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      1.00      0.99      1386\\n           1       0.99      0.97      0.98       390\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.98      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "cohen=[]\n",
    "score=[]\n",
    "confusion=[]\n",
    "class_out=[]\n",
    "for i in range(10): \n",
    "    model=RandomForestClassifier()\n",
    "    x=total_wine.drop(columns='wine_type')\n",
    "    y=total_wine['wine_type']\n",
    "    x_train,x_test,y_train, y_test=train_test_split(x,y, test_size=0.3)\n",
    "    scale=MinMaxScaler()\n",
    "    x_train_n= scale.fit_transform(x_train)\n",
    "    x_train_n= pd.DataFrame(x_train_n, columns=x_train.columns)\n",
    "    x_test_n= scale.fit_transform(x_test)\n",
    "    x_test_n= pd.DataFrame(x_test_n, columns=x_test.columns)\n",
    "    model.fit(x_train_n, y_train)\n",
    "    pred=model.predict(x_test_n)\n",
    "    scor=accuracy_score(pred, y_test)\n",
    "    score.append(scor)\n",
    "\n",
    "    con=confusion_matrix(y_test, pred)\n",
    "    confusion.append(con)\n",
    "    class_p= classification_report(y_test,pred)\n",
    "    class_out.append(class_p)\n",
    "    cohe=cohen_kappa_score(y_test, pred)\n",
    "    cohen.append(cohe)\n",
    "    i +=1\n",
    "print('score :\\n', np.mean(score),'\\ncohen: \\n',np.mean(cohen),'\\nconfusion \\n',confusion, 'classification: \\n',class_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score :\n",
      " 0.995045045045045 \n",
      "cohen: \n",
      " 0.9853678207910574 \n",
      "confusion \n",
      " [array([[1417,    5],\n",
      "       [   4,  350]]), array([[1411,    2],\n",
      "       [   4,  359]]), array([[1391,    2],\n",
      "       [   6,  377]]), array([[1401,    3],\n",
      "       [   3,  369]]), array([[1380,    4],\n",
      "       [   3,  389]]), array([[1357,    8],\n",
      "       [   7,  404]]), array([[1413,    3],\n",
      "       [   6,  354]]), array([[1376,    9],\n",
      "       [   5,  386]]), array([[1396,    1],\n",
      "       [   4,  375]]), array([[1363,    4],\n",
      "       [   5,  404]])] classification: \n",
      " ['              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1422\\n           1       0.99      0.99      0.99       354\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1413\\n           1       0.99      0.99      0.99       363\\n\\n    accuracy                           1.00      1776\\n   macro avg       1.00      0.99      0.99      1776\\nweighted avg       1.00      1.00      1.00      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1393\\n           1       0.99      0.98      0.99       383\\n\\n    accuracy                           1.00      1776\\n   macro avg       1.00      0.99      0.99      1776\\nweighted avg       1.00      1.00      1.00      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1404\\n           1       0.99      0.99      0.99       372\\n\\n    accuracy                           1.00      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       1.00      1.00      1.00      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1384\\n           1       0.99      0.99      0.99       392\\n\\n    accuracy                           1.00      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       1.00      1.00      1.00      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      0.99      0.99      1365\\n           1       0.98      0.98      0.98       411\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1416\\n           1       0.99      0.98      0.99       360\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      0.99      0.99      1385\\n           1       0.98      0.99      0.98       391\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1397\\n           1       1.00      0.99      0.99       379\\n\\n    accuracy                           1.00      1776\\n   macro avg       1.00      0.99      1.00      1776\\nweighted avg       1.00      1.00      1.00      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1367\\n           1       0.99      0.99      0.99       409\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "cohen=[]\n",
    "score=[]\n",
    "confusion=[]\n",
    "class_out=[]\n",
    "for i in range(10): \n",
    "    model=LinearDiscriminantAnalysis()\n",
    "    x=total_wine.drop(columns='wine_type')\n",
    "    y=total_wine['wine_type']\n",
    "    x_train,x_test,y_train, y_test=train_test_split(x,y, test_size=0.3)\n",
    "    scale=MinMaxScaler()\n",
    "    x_train_n= scale.fit_transform(x_train)\n",
    "    x_train_n= pd.DataFrame(x_train_n, columns=x_train.columns)\n",
    "    x_test_n= scale.fit_transform(x_test)\n",
    "    x_test_n= pd.DataFrame(x_test_n, columns=x_test.columns)\n",
    "    model.fit(x_train_n, y_train)\n",
    "    pred=model.predict(x_test_n)\n",
    "    scor=accuracy_score(pred, y_test)\n",
    "    score.append(scor)\n",
    "\n",
    "    con=confusion_matrix(y_test, pred)\n",
    "    confusion.append(con)\n",
    "    class_p= classification_report(y_test,pred)\n",
    "    class_out.append(class_p)\n",
    "    cohe=cohen_kappa_score(y_test, pred)\n",
    "    cohen.append(cohe)\n",
    "    i +=1\n",
    "print('score :\\n', np.mean(score),'\\ncohen: \\n',np.mean(cohen),'\\nconfusion \\n',confusion, 'classification: \\n',class_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score :\n",
      " 0.9848536036036037 \n",
      "cohen: \n",
      " 0.9552877410091968 \n",
      "confusion \n",
      " [array([[1374,   17],\n",
      "       [  12,  373]]), array([[1386,   12],\n",
      "       [  14,  364]]), array([[1378,   17],\n",
      "       [  13,  368]]), array([[1359,   18],\n",
      "       [  12,  387]]), array([[1361,   10],\n",
      "       [  14,  391]]), array([[1373,   17],\n",
      "       [  13,  373]]), array([[1403,   19],\n",
      "       [   6,  348]]), array([[1358,   17],\n",
      "       [   5,  396]]), array([[1383,   22],\n",
      "       [   3,  368]]), array([[1404,   17],\n",
      "       [  11,  344]])] classification: \n",
      " ['              precision    recall  f1-score   support\\n\\n           0       0.99      0.99      0.99      1391\\n           1       0.96      0.97      0.96       385\\n\\n    accuracy                           0.98      1776\\n   macro avg       0.97      0.98      0.98      1776\\nweighted avg       0.98      0.98      0.98      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      0.99      0.99      1398\\n           1       0.97      0.96      0.97       378\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.98      0.98      0.98      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      0.99      0.99      1395\\n           1       0.96      0.97      0.96       381\\n\\n    accuracy                           0.98      1776\\n   macro avg       0.97      0.98      0.98      1776\\nweighted avg       0.98      0.98      0.98      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      0.99      0.99      1377\\n           1       0.96      0.97      0.96       399\\n\\n    accuracy                           0.98      1776\\n   macro avg       0.97      0.98      0.98      1776\\nweighted avg       0.98      0.98      0.98      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      0.99      0.99      1371\\n           1       0.98      0.97      0.97       405\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.98      0.98      0.98      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      0.99      0.99      1390\\n           1       0.96      0.97      0.96       386\\n\\n    accuracy                           0.98      1776\\n   macro avg       0.97      0.98      0.98      1776\\nweighted avg       0.98      0.98      0.98      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      0.99      0.99      1422\\n           1       0.95      0.98      0.97       354\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.97      0.98      0.98      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      0.99      0.99      1375\\n           1       0.96      0.99      0.97       401\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.98      0.99      0.98      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      0.98      0.99      1405\\n           1       0.94      0.99      0.97       371\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.97      0.99      0.98      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      0.99      0.99      1421\\n           1       0.95      0.97      0.96       355\\n\\n    accuracy                           0.98      1776\\n   macro avg       0.97      0.98      0.98      1776\\nweighted avg       0.98      0.98      0.98      1776\\n']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "cohen=[]\n",
    "score=[]\n",
    "confusion=[]\n",
    "class_out=[]\n",
    "for i in range(10): \n",
    "    model=GaussianNB()\n",
    "    x=total_wine.drop(columns='wine_type')\n",
    "    y=total_wine['wine_type']\n",
    "    x_train,x_test,y_train, y_test=train_test_split(x,y, test_size=0.3)\n",
    "    scale=MinMaxScaler()\n",
    "    x_train_n= scale.fit_transform(x_train)\n",
    "    x_train_n= pd.DataFrame(x_train_n, columns=x_train.columns)\n",
    "    x_test_n= scale.fit_transform(x_test)\n",
    "    x_test_n= pd.DataFrame(x_test_n, columns=x_test.columns)\n",
    "    model.fit(x_train_n, y_train)\n",
    "    pred=model.predict(x_test_n)\n",
    "    scor=accuracy_score(pred, y_test)\n",
    "    score.append(scor)\n",
    "\n",
    "    con=confusion_matrix(y_test, pred)\n",
    "    confusion.append(con)\n",
    "    class_p= classification_report(y_test,pred)\n",
    "    class_out.append(class_p)\n",
    "    cohe=cohen_kappa_score(y_test, pred)\n",
    "    cohen.append(cohe)\n",
    "    i +=1\n",
    "print('score :\\n', np.mean(score),'\\ncohen: \\n',np.mean(cohen),'\\nconfusion \\n',confusion, 'classification: \\n',class_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marziehbaes/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/Users/marziehbaes/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/Users/marziehbaes/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/Users/marziehbaes/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/Users/marziehbaes/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/Users/marziehbaes/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/Users/marziehbaes/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/Users/marziehbaes/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score :\n",
      " 0.9936936936936936 \n",
      "cohen: \n",
      " 0.9814034503869491 \n",
      "confusion \n",
      " [array([[1396,    3],\n",
      "       [   5,  372]]), array([[1376,    6],\n",
      "       [   5,  389]]), array([[1402,    2],\n",
      "       [   4,  368]]), array([[1399,    3],\n",
      "       [  10,  364]]), array([[1370,    5],\n",
      "       [   8,  393]]), array([[1415,    5],\n",
      "       [   4,  352]]), array([[1387,    5],\n",
      "       [   7,  377]]), array([[1370,    4],\n",
      "       [   7,  395]]), array([[1397,    4],\n",
      "       [   9,  366]]), array([[1363,    4],\n",
      "       [  12,  397]])] classification: \n",
      " ['              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1399\\n           1       0.99      0.99      0.99       377\\n\\n    accuracy                           1.00      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       1.00      1.00      1.00      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1382\\n           1       0.98      0.99      0.99       394\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1404\\n           1       0.99      0.99      0.99       372\\n\\n    accuracy                           1.00      1776\\n   macro avg       1.00      0.99      0.99      1776\\nweighted avg       1.00      1.00      1.00      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      1.00      1.00      1402\\n           1       0.99      0.97      0.98       374\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      1.00      1.00      1375\\n           1       0.99      0.98      0.98       401\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1420\\n           1       0.99      0.99      0.99       356\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      1.00      1.00      1392\\n           1       0.99      0.98      0.98       384\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      1.00      1.00      1374\\n           1       0.99      0.98      0.99       402\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      1.00      1.00      1401\\n           1       0.99      0.98      0.98       375\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      1.00      0.99      1367\\n           1       0.99      0.97      0.98       409\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.98      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marziehbaes/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/Users/marziehbaes/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "cohen=[]\n",
    "score=[]\n",
    "confusion=[]\n",
    "class_out=[]\n",
    "for i in range(10): \n",
    "    model=KNeighborsClassifier()\n",
    "    x=total_wine.drop(columns='wine_type')\n",
    "    y=total_wine['wine_type']\n",
    "    x_train,x_test,y_train, y_test=train_test_split(x,y, test_size=0.3)\n",
    "    scale=MinMaxScaler()\n",
    "    x_train_n= scale.fit_transform(x_train)\n",
    "    x_train_n= pd.DataFrame(x_train_n, columns=x_train.columns)\n",
    "    x_test_n= scale.fit_transform(x_test)\n",
    "    x_test_n= pd.DataFrame(x_test_n, columns=x_test.columns)\n",
    "    model.fit(x_train_n, y_train)\n",
    "    pred=model.predict(x_test_n)\n",
    "    scor=accuracy_score(pred, y_test)\n",
    "    score.append(scor)\n",
    "\n",
    "    con=confusion_matrix(y_test, pred)\n",
    "    confusion.append(con)\n",
    "    class_p= classification_report(y_test,pred)\n",
    "    class_out.append(class_p)\n",
    "    cohe=cohen_kappa_score(y_test, pred)\n",
    "    cohen.append(cohe)\n",
    "    i +=1\n",
    "print('score :\\n', np.mean(score),'\\ncohen: \\n',np.mean(cohen),'\\nconfusion \\n',confusion, 'classification: \\n',class_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score :\n",
      " 0.9956644144144142 \n",
      "cohen: \n",
      " 0.9869987748021151 \n",
      "confusion \n",
      " [array([[1406,    1],\n",
      "       [   6,  363]]), array([[1399,    1],\n",
      "       [  11,  365]]), array([[1379,    2],\n",
      "       [   4,  391]]), array([[1383,    2],\n",
      "       [   8,  383]]), array([[1396,    1],\n",
      "       [   4,  375]]), array([[1398,    4],\n",
      "       [   6,  368]]), array([[1395,    1],\n",
      "       [   5,  375]]), array([[1400,    3],\n",
      "       [   4,  369]]), array([[1409,    4],\n",
      "       [   4,  359]]), array([[1394,    1],\n",
      "       [   5,  376]])] classification: \n",
      " ['              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1407\\n           1       1.00      0.98      0.99       369\\n\\n    accuracy                           1.00      1776\\n   macro avg       1.00      0.99      0.99      1776\\nweighted avg       1.00      1.00      1.00      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      1.00      1.00      1400\\n           1       1.00      0.97      0.98       376\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1381\\n           1       0.99      0.99      0.99       395\\n\\n    accuracy                           1.00      1776\\n   macro avg       1.00      0.99      1.00      1776\\nweighted avg       1.00      1.00      1.00      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      1.00      1.00      1385\\n           1       0.99      0.98      0.99       391\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1397\\n           1       1.00      0.99      0.99       379\\n\\n    accuracy                           1.00      1776\\n   macro avg       1.00      0.99      1.00      1776\\nweighted avg       1.00      1.00      1.00      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1402\\n           1       0.99      0.98      0.99       374\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1396\\n           1       1.00      0.99      0.99       380\\n\\n    accuracy                           1.00      1776\\n   macro avg       1.00      0.99      0.99      1776\\nweighted avg       1.00      1.00      1.00      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1403\\n           1       0.99      0.99      0.99       373\\n\\n    accuracy                           1.00      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       1.00      1.00      1.00      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1413\\n           1       0.99      0.99      0.99       363\\n\\n    accuracy                           1.00      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       1.00      1.00      1.00      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1395\\n           1       1.00      0.99      0.99       381\\n\\n    accuracy                           1.00      1776\\n   macro avg       1.00      0.99      0.99      1776\\nweighted avg       1.00      1.00      1.00      1776\\n']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "cohen=[]\n",
    "score=[]\n",
    "confusion=[]\n",
    "class_out=[]\n",
    "for i in range(10): \n",
    "    model=SVC()\n",
    "    x=total_wine.drop(columns='wine_type')\n",
    "    y=total_wine['wine_type']\n",
    "    x_train,x_test,y_train, y_test=train_test_split(x,y, test_size=0.3)\n",
    "    scale=MinMaxScaler()\n",
    "    x_train_n= scale.fit_transform(x_train)\n",
    "    x_train_n= pd.DataFrame(x_train_n, columns=x_train.columns)\n",
    "    x_test_n= scale.fit_transform(x_test)\n",
    "    x_test_n= pd.DataFrame(x_test_n, columns=x_test.columns)\n",
    "    model.fit(x_train_n, y_train)\n",
    "    pred=model.predict(x_test_n)\n",
    "    scor=accuracy_score(pred, y_test)\n",
    "    score.append(scor)\n",
    "\n",
    "    con=confusion_matrix(y_test, pred)\n",
    "    confusion.append(con)\n",
    "    class_p= classification_report(y_test,pred)\n",
    "    class_out.append(class_p)\n",
    "    cohe=cohen_kappa_score(y_test, pred)\n",
    "    cohen.append(cohe)\n",
    "    i +=1\n",
    "print('score :\\n', np.mean(score),'\\ncohen: \\n',np.mean(cohen),'\\nconfusion \\n',confusion, 'classification: \\n',class_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score :\n",
      " 0.9920045045045045 \n",
      "cohen: \n",
      " 0.976336511153556 \n",
      "confusion \n",
      " [array([[1388,    5],\n",
      "       [   9,  374]]), array([[1368,    7],\n",
      "       [   7,  394]]), array([[1397,    9],\n",
      "       [   7,  363]]), array([[1402,    2],\n",
      "       [  10,  362]]), array([[1373,    3],\n",
      "       [  18,  382]]), array([[1397,    5],\n",
      "       [   9,  365]]), array([[1396,    5],\n",
      "       [   7,  368]]), array([[1379,    6],\n",
      "       [   6,  385]]), array([[1397,    6],\n",
      "       [   4,  369]]), array([[1375,   11],\n",
      "       [   6,  384]])] classification: \n",
      " ['              precision    recall  f1-score   support\\n\\n           0       0.99      1.00      0.99      1393\\n           1       0.99      0.98      0.98       383\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      0.99      0.99      1375\\n           1       0.98      0.98      0.98       401\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      0.99      0.99      1406\\n           1       0.98      0.98      0.98       370\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      1.00      1.00      1404\\n           1       0.99      0.97      0.98       372\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      1.00      0.99      1376\\n           1       0.99      0.95      0.97       400\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.98      0.98      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       0.99      1.00      1.00      1402\\n           1       0.99      0.98      0.98       374\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1401\\n           1       0.99      0.98      0.98       375\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1385\\n           1       0.98      0.98      0.98       391\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00      1403\\n           1       0.98      0.99      0.99       373\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.99      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n', '              precision    recall  f1-score   support\\n\\n           0       1.00      0.99      0.99      1386\\n           1       0.97      0.98      0.98       390\\n\\n    accuracy                           0.99      1776\\n   macro avg       0.98      0.99      0.99      1776\\nweighted avg       0.99      0.99      0.99      1776\\n']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "cohen=[]\n",
    "score=[]\n",
    "confusion=[]\n",
    "class_out=[]\n",
    "for i in range(10): \n",
    "    model=AdaBoostClassifier()\n",
    "    x=total_wine.drop(columns='wine_type')\n",
    "    y=total_wine['wine_type']\n",
    "    x_train,x_test,y_train, y_test=train_test_split(x,y, test_size=0.3)\n",
    "    scale=MinMaxScaler()\n",
    "    x_train_n= scale.fit_transform(x_train)\n",
    "    x_train_n= pd.DataFrame(x_train_n, columns=x_train.columns)\n",
    "    x_test_n= scale.fit_transform(x_test)\n",
    "    x_test_n= pd.DataFrame(x_test_n, columns=x_test.columns)\n",
    "    model.fit(x_train_n, y_train)\n",
    "    pred=model.predict(x_test_n)\n",
    "    scor=accuracy_score(pred, y_test)\n",
    "    score.append(scor)\n",
    "\n",
    "    con=confusion_matrix(y_test, pred)\n",
    "    confusion.append(con)\n",
    "    class_p= classification_report(y_test,pred)\n",
    "    class_out.append(class_p)\n",
    "    cohe=cohen_kappa_score(y_test, pred)\n",
    "    cohen.append(cohe)\n",
    "    i +=1\n",
    "print('score :\\n', np.mean(score),'\\ncohen: \\n',np.mean(cohen),'\\nconfusion \\n',confusion, 'classification: \\n',class_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2918784d2acf282f2c1eeb3293c1f68c9e526abdf8983671e7e8ea84935e28a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
